/*
 * ----------------------------------------------------------------------------
 * LAB 2
 * Kernel Programming 2018 @VU
 * by Matthijs Jansen and Zuzana Hromcova
 * Version from Friday, September 14th
 * ----------------------------------------------------------------------------
 * 
 * Remarks:
 * 
 * Huge pages are implemented almost the same as small pages, with the
 * exception that their descriptive entry is located in PD instead of PTE.
 * 
 * page_walk() - We walk the page table tree and return a PTE or PDE
 * (depends on whether we query a small or a huge page).
 * 
 * page_insert() - We create a mapping for the given virtual page (remove
 * it first if it was already mapped). We also remove the page from the
 * free list if it was there (to avoid randomly allocating it later).
 * 
 * page_remove() - We clear the PTE/PDE, flush TLB cache and
 * decrement refcount on that page.
 * 
 * boot_map_kernel() - We exclude program segments with addresses below
 * KERNEL_VMA from the mapping.
 * 
 * NX bit - The NX is enabled using the provided function. The PAGE_WRITE | 
 * PAGE_NO_EXEC are setup when the pages are allocated.
 * 
 */

/* See COPYRIGHT for copyright information. */

#include <inc/error.h>
#include <inc/string.h>
#include <inc/assert.h>
#include <inc/paging.h>
#include <inc/elf.h>

#include <inc/x86-64/asm.h>

#include <kern/env.h>
#include <kern/pmap.h>
#include <kern/lab1.c>

/* These variables are set in mem_init() */
size_t npages;
struct page_table *kern_pml4;           /* Kernel's initial PML4 */
struct page_info *pages;                /* Physical page state array */
struct page_info *page_free_list;       /* Free list of physical pages */

/***************************************************************
 * Set up memory mappings above UTOP.
 ***************************************************************/

static void boot_map_region(struct page_table *pml4, uintptr_t va, size_t size,
    physaddr_t pa, uint64_t perm);
static void boot_map_kernel(struct elf *elf_hdr);
static physaddr_t *page_walk(struct page_table *pml4, const void *va, int create);
static void check_page_free_list(bool only_low_memory);
static void check_page_alloc(void);
static void check_kern_pml4(void);
static physaddr_t check_va2pa(struct page_table *pml4, uintptr_t va);
static void check_page(void);
static void check_page_hugepages(void);
static void check_page_installed_pml4(void);
static void check_wx(void);

/* This simple physical memory allocator is used only while JOS is setting up
 * its virtual memory system.  page_alloc() is the real allocator.
 *
 * If n>0, allocates enough pages of contiguous physical memory to hold 'n'
 * bytes.  Doesn't initialize the memory.  Returns a kernel virtual address.
 *
 * If n==0, returns the address of the next free page without allocating
 * anything.
 *
 * If we're out of memory, boot_alloc should panic.
 * This function may ONLY be used during initialization, before the
 * page_free_list list has been set up. */
static void *boot_alloc(uint32_t n)
{
    static char *nextfree;  /* virtual address of next byte of free memory */
    char *result;
    unsigned long long virt_max = KERNEL_VMA + 0x100000000;

    /* Initialize nextfree if this is the first time. 'end' is a magic symbol
     * automatically generated by the linker, which points to the end of the
     * kernel's bss segment: the first virtual address that the linker did *not*
     * assign to any kernel code or global variables. */
    if (!nextfree) {
        extern char end[];
        nextfree = ROUNDUP((char *)end, PAGE_SIZE);
    }

    /* Allocate a chunk large enough to hold 'n' bytes, then update nextfree.
     * Make sure nextfree is kept aligned to a multiple of PAGE_SIZE.
     *
     * LAB 1: Your code here.
     */
    result = nextfree;
    nextfree = ROUNDUP(nextfree + n, PAGE_SIZE);

    // Out of memory
    if ((unsigned long long)nextfree >= virt_max) {
        panic("Out of virtual memory in boot_alloc\n");
    }

    return result;
}

/*
 * Set up a 4-level page table:
 *    kern_pml4 is its linear (virtual) address of the root
 *
 * This function only sets up the kernel part of the address space (ie.
 * addresses >= UTOP).  The user part of the address space will be setup later.
 *
 * From UTOP to ULIM, the user is allowed to read but not write.
 * Above ULIM the user cannot read or write.
 */
void mem_init(struct boot_info *boot_info)
{
    struct mmap_entry *entry;
    uintptr_t highest_addr = 0;
    uint32_t cr0;
    size_t i, n;

    //----------------------------------------------------------------------------------
    size_t j, vma_list_size;
    struct vma *vma_list;
    struct vma *vma_tmp;
    //----------------------------------------------------------------------------------

    cprintf("[MEM_INIT] START\n");

    /* Find the amount of pages to allocate structs for. */
    entry = (struct mmap_entry *)((physaddr_t)boot_info->mmap_addr);

    for (i = 0; i < boot_info->mmap_len; ++i, ++entry) {
        if (entry->type != MMAP_FREE)
            continue;

        highest_addr = entry->addr + entry->len;
    }

    npages = highest_addr / PAGE_SIZE;

    /*********************************************************************
     * Create the initial page tables.
     */
    kern_pml4 = boot_alloc(sizeof *kern_pml4);
    memset(kern_pml4, 0, sizeof *kern_pml4);

    /*********************************************************************
     * Recursively insert PD in itself as a page table, to form a virtual page
     * table at virtual address USER_PML4.
     * (For now, you don't have understand the greater purpose of the following
     * line.)
     * Permissions: kernel R, user R.
     */
    kern_pml4->entries[PML4_INDEX(USER_PML4)] =
        PADDR(kern_pml4) | PAGE_PRESENT | PAGE_USER | PAGE_NO_EXEC;

    /*********************************************************************
     * Allocate an array of npages 'struct page_info's and store it in 'pages'.
     * The kernel uses this array to keep track of physical pages: for each
     * physical page, there is a corresponding struct page_info in this array.
     * 'npages' is the number of physical pages in memory.  Your code goes here.
     */
    pages = boot_alloc(sizeof(struct page_info)*npages);
    
    for (i = 0; i < npages; i++) {
        pages[i].pp_link = NULL;
        pages[i].previous = NULL;
        pages[i].pp_ref = 0;
        pages[i].is_huge = 0;
        pages[i].is_available = 0;
    }

     /*********************************************************************
     * Make 'envs' point to an array of size 'NENV' of 'struct env'.
     * LAB 3: your code here.
     */

    envs = boot_alloc(sizeof(struct env)*NENV);

    //-----------------------------------------------------------------------------------------
    cprintf("before allocating VMA\n");
    for (i = 0; i < NENV; i++) {
        vma_list = boot_alloc(sizeof(struct vma)*128);
        envs[i].vma = vma_list;
    }
    cprintf("after allocating VMA\n");
    //-----------------------------------------------------------------------------------------

    /*********************************************************************
     * Now that we've allocated the initial kernel data structures, we set
     * up the list of free physical pages. Once we've done so, all further
     * memory management will go through the page_* functions. In particular, we
     * can now map memory using boot_map_region or page_insert.
     */

    page_init(boot_info);
    check_page_free_list(1);
    check_page_alloc();
    check_page();

    /*********************************************************************
     * Now we set up virtual memory.
     *********************************************************************/

    /*********************************************************************
     * Map 'pages' read-only by the user at linear address USER_PAGES
     * Permissions:
     *    - the new image at USER_PAGES -- kernel R, user R
     *      (ie. perm = PAGE_USER | PAGE_PRESENT)
     *    - pages itself -- kernel RW, user NONE
     * Your code goes here:
   */
    boot_map_region(kern_pml4, USER_PAGES, 
        ROUNDUP(npages * sizeof(struct page_info), PAGE_SIZE),
        PADDR(pages), PAGE_WRITE | PAGE_NO_EXEC);

    /*********************************************************************
     * Map the 'envs' array read-only by the user at linear address UENVS
     * (ie. perm = PTE_U | PTE_P). User | Present
     * Permissions:
     *    - the new image at UENVS  -- kernel R, user R
     *    - envs itself -- kernel RW, user NONE
     * LAB 3: your code here.
     */
    boot_map_region(kern_pml4, USER_ENVS, 
        ROUNDUP(NENV * sizeof(struct env), PAGE_SIZE),
        PADDR(envs), PAGE_WRITE | PAGE_NO_EXEC | PAGE_USER);

    physaddr_t *addr;
    addr = page_walk(kern_pml4, (void *)USER_ENVS, 0);

    /*********************************************************************
     * Map the 'VMAs' array as kernel RW, user NONE
     */
    vma_list_size = ROUNDUP(128 * sizeof(struct vma), PAGE_SIZE);

    for (i = 0; i < NENV; i++) {
        boot_map_region(kern_pml4, USER_VMAS + (i)*vma_list_size, vma_list_size,
            PADDR(envs[i].vma), PAGE_WRITE | PAGE_NO_EXEC);
    }

    /*********************************************************************
     * Use the physical memory that 'bootstack' refers to as the kernel
     * stack. The kernel stack grows down from virtual address KSTACK_TOP.
     * We consider the entire range from [KSTACK_TOP-PTSIZE, KSTACK_TOP)
     * to be the kernel stack, but break this into two pieces:
     *     * [KSTACK_TOP-KSTACK_SIZE, KSTACK_TOP) -- backed by physical memory
     *     * [KSTACK_TOP-PTSIZE, KSTACK_TOP-KSTACK_SIZE) -- not backed; so if
     *       the kernel overflows its stack, it will fault rather than
     *       overwrite memory.  Known as a "guard page".
     *     Permissions: kernel RW, user NONE
     *
     * Note: don't map anything between KSTACKTOP - PTSIZE and
     * KSTACKTOP - KTSIZE leaving this as guard region.
     *
     * Your code goes here:
     */
    uintptr_t vi;
    boot_map_region(kern_pml4, KSTACK_TOP-KSTACK_SIZE, KSTACK_SIZE, 
                    (physaddr_t)bootstack, PAGE_WRITE | PAGE_NO_EXEC);

    for (vi = KSTACK_TOP-KSTACK_SIZE-KSTACK_GAP; vi < KSTACK_TOP-KSTACK_SIZE; 
         vi += PAGE_SIZE) {
        page_walk(kern_pml4, (void *)vi, CREATE_NORMAL);
    }

    /*********************************************************************
     * Map all of physical memory at KERNBASE.
     * Ie.  the VA range [KERNBASE, 2^32) should map to
     *      the PA range [0, 2^32 - KERNBASE)
     * We might not have 2^32 - KERNBASE bytes of physical memory, but
     * we just set up the mapping anyway.
     * Permissions: kernel RW, user NONE
     * Your code goes here:
     */
    boot_map_kernel((struct elf *)(KERNEL_VMA + (uintptr_t)boot_info->elf_hdr));

    /* Check that the initial page directory has been set up correctly. */
    cprintf("[CHECK_KERN_PML4] START - W3 ENV CHECK\n");
    check_kern_pml4();
    cprintf("[CHECK_KERN_PML4] END   - W3 ENV CHECK\n");

    /* Enable the NX-bit. */
    write_msr(MSR_EFER, MSR_EFER_NXE);


    /* Switch from the minimal entry page directory to the full kern_pml4
     * page table we just created.  Our instruction pointer should be
     * somewhere between KERNBASE and KERNBASE+4MB right now, which is
     * mapped the same way by both page tables.
     *
     * If the machine reboots at this point, you've probably set up your
     * kern_pml4 wrong. */
    load_pml4((void *)PADDR(kern_pml4));

    check_page_free_list(0);

    /* Some more checks, only possible after kern_pml4 is installed. */
    check_page_installed_pml4();
 
    /* Check if the kernel page tables fulfil W^X. */
    check_wx();

    check_page_hugepages();

    boot_map_region(kern_pml4, KERNEL_VMA, 0x100000000, 0, PAGE_WRITE);
    cprintf("[MEM_INIT] END\n");
}

/***************************************************************
 * Tracking of physical pages.
 * The 'pages' array has one 'struct page_info' entry per physical page.
 * Pages are reference counted, and free pages are kept on a linked list.
 ***************************************************************/

/*
 * Initialize page structure and memory free list.
 * After this is done, NEVER use boot_alloc again.  ONLY use the page
 * allocator functions below to allocate and deallocate physical
 * memory via the page_free_list.
 */
void page_init(struct boot_info *boot_info)
{
    struct page_info *page;
    struct mmap_entry *entry;
    uintptr_t pa, end;
    pa = 0;
    size_t i;
    struct elf *elf_hdr = (struct elf *)(
        KERNEL_VMA + (uintptr_t)boot_info->elf_hdr);

    /*
     * The example code here marks all physical pages as free. However this is
     * not truly the case. What memory is free?
     *
     * The boot loader requests the memory map from the BIOS. This memory map
     * consists of multiple entries of which each spans a region of physical
     * memory and describes whether is free or not.
     *
     * Furthermore, we have to mark the following regions as used:
     *  1) Physical page 0 to preserve the real-mode IVT and BIOS structures in
     *     in case we need them.
     *  2) The memory used by the kernel. Where is the kernel located in
     *     physical memory? How much is in use by the point we get here?
     *  3) The boot loader also loaded in the ELF header structures of the
     *     kernel. We have to preserve these headers until we have set up the
     *     right permissions in the page tables.
     * Change the code to reflect this.
     * NB: DO NOT actually touch the physical memory corresponding to free
     *     pages! */
    entry = (struct mmap_entry *)KADDR(boot_info->mmap_addr);
    end = PADDR(boot_alloc(0));

    page_free_list = NULL;
    for (i = 0; i < boot_info->mmap_len; ++i, ++entry) {
        if (entry->type != MMAP_FREE) {
            continue;
        }
        for (pa = entry->addr; pa < entry->addr + entry->len; pa += PAGE_SIZE) {
            page = pa2page(pa);

            if (!(pa == 0 || pa == MPENTRY_PADDR || 
                 (pa >= KERNEL_LMA && pa < end) ||
                  pa == PADDR(elf_hdr))) {
                page->is_available = 1;

                // Set next and previous
                page->pp_link = page_free_list;
                if (page_free_list != NULL) {
                    page_free_list->previous = page;
                }
                page_free_list = page;
            } 
        }
    }

    // Merge the small pages to create huge pages
    initial_merge(boot_info);
}

/*
 * Allocates a physical page.  If (alloc_flags & ALLOC_ZERO), fills the entire
 * returned physical page with '\0' bytes.  Does NOT increment the reference
 * count of the page - the caller must do these if necessary (either explicitly
 * or via page_insert).
 *
 * Be sure to set the pp_link field of the allocated page to NULL so
 * page_free can check for double-free bugs.
 *
 * Returns NULL if out of free memory.
 *
 * Hint: use page2kva and memset
 *
 * 2MB huge pages:
 * Come back later to extend this function to support 2MB huge page allocation.
 * if (alloc_flags & ALLOC_HUGE), returns a huge physical page of 2MB size.
 */
struct page_info *page_alloc(int alloc_flags)
{
    struct page_info *page;
    struct page_info *tmp;
    int i;
    
    // If out of memory, return NULL
    if (page_free_list == NULL) {
        return NULL;
    }

    // Try to allocate a huge page
    if (alloc_flags & ALLOC_HUGE) {
        page =  delete_from_free(1);
        if (page == NULL)
            return NULL;
    }
    // Try to allocate a small page
    else {
        page = delete_from_free(0);

        // If there is no small page available,
        // we will split a huge page into several small pages
        if (page == NULL) {
            // Save first page
            tmp = page_free_list;
            page_free_list = page_free_list->pp_link;

            if (page_free_list != NULL) {
                page_free_list->previous = NULL;
            }

            tmp->is_huge = 0;
            tmp->is_available = 0;
            tmp->pp_link = NULL;
            tmp->previous = NULL;

            // Add the rest of the pages to the free pages pool.
            // They are stored in an array so we can do it like this.
            page = tmp;
            for (i = 1; i < SMALL_PAGES_IN_HUGE; i++) {
                // Prepend
                page[i].pp_link = page_free_list;
                if (page_free_list != NULL) {
                    page_free_list->previous = &page[i];
                } 
                page_free_list = &page[i];

                page[i].is_huge = 0;
                page[i].is_available = 1;
            }
            page_free_list->previous = NULL;

            // First page of huge page is allocated
            page = tmp;
        }
    }

    // Initialize with zeros
    if (alloc_flags & ALLOC_ZERO) {
        if (alloc_flags & ALLOC_HUGE)
            memset(page2kva(page), 0, PAGE_SIZE * SMALL_PAGES_IN_HUGE);
        else
            memset(page2kva(page), 0, PAGE_SIZE);
    }

    return page;
}

/*
 * Return a page to the free list.
 * (This function should only be called when pp->pp_ref reaches 0.)
 */
void page_free(struct page_info *pp)
{
    struct page_info* tmp;
    struct page_info* prev;
    int i, j;
    j = 0;
    
    /* Hint: You may want to panic if pp->pp_ref is nonzero or
     * pp->pp_link is not NULL. */
    if (pp->pp_link != NULL) {
        panic("Failed to free a page with pp_link != NULL");
    }
    if (pp->pp_ref != 0) {
        panic("Failed to free a page with nonzero refcount");
    }
    if (pp->is_available == 1) {
        panic("Attempt to double-free a page failed");
    }

    // Add page to free list
    if (page_free_list == NULL) {
        page_free_list = pp;
        pp->pp_link = NULL;
    } else {
        page_free_list->previous = pp;
        pp->pp_link = page_free_list;
        page_free_list = pp;
    }
    pp->previous = NULL;
    pp->is_available = 1;

    // If small page is added, check if huge page can be created
    if (!pp->is_huge) {
        merge_after_free(pp);
    }
}

/*
 * Decrement the reference count on a page,
 * freeing it if there are no more refs.
 */
void page_decref(struct page_info* pp)
{
    if (--pp->pp_ref == 0)
        page_free(pp);
}

/*
 * Map [va, va+size) of virtual address space to physical [pa, pa+size)
 * in the page table rooted at pgdir.  Size is a multiple of PAGE_SIZE.
 * Use permission bits perm|PAGE_PRESENT for the entries.
 *
 * This function is only intended to set up the ``static'' mappings
 * above UTOP. As such, it should *not* change the pp_ref field on the
 * mapped pages.
 *
 * Hint: the TA solution uses page_walk
 */
static void boot_map_region(struct page_table *pml4, uintptr_t va, size_t size,
    physaddr_t pa, uint64_t perm)
{
    uintptr_t vi, pi;
    physaddr_t *addr;

    for (vi = va, pi = pa; vi < va+size; vi += PAGE_SIZE, pi += PAGE_SIZE) {
        addr = page_walk(pml4, (void *)vi, CREATE_NORMAL);
        *addr = pi | perm | PAGE_PRESENT;
    }
}

/*
 * Map the identity mapping as RW and parse the ELF header of the kernel and
 * map the program segments with the appropriate permissions.
 */
static void boot_map_kernel(struct elf *elf_hdr)
{
    struct elf_proghdr *prog_hdr, next;
    prog_hdr = (struct elf_proghdr *)((char *)elf_hdr + elf_hdr->e_phoff);
    int i = 0;
    long int flags;

    // Identity mapping, kernel RW, user None
    boot_map_region(kern_pml4, KERNEL_VMA, npages * PAGE_SIZE, 0, 
        PAGE_WRITE | PAGE_NO_EXEC);

    // Loop through all segments
    for (i = 0; i < elf_hdr->e_phnum; i++) {
        next = prog_hdr[i];
        if (next.p_type != ELF_PROG_LOAD) {
            continue;
        }
        if (next.p_va <= KERNEL_VMA) {
            continue;
        }

        flags = next.p_flags;
        if (flags & PAGE_WRITE) {
            flags |= PAGE_NO_EXEC;
        }
        
        boot_map_region(kern_pml4, next.p_va, ROUNDUP(next.p_memsz, PAGE_SIZE), 
                        next.p_pa, flags);
    }
}

/* Check if 'entry' is an entry pointing to a table or still empty
 * If entry is already valid, return entry.
 * If not, create a new table and let entry point to it
 */
int entry_in_table(physaddr_t *entry, int create) 
{   
    struct page_table *new;
    struct page_info *page;

    // Check if entry is empty or not
    if (!(*entry & PAGE_PRESENT)) {
        if (!create) {
            return 0;
        }
        // Create a lower level entry in the higher table and thus
        // a new lower level table itself too
        // New entry: kernel R, user R
        else {
            page = page_alloc(ALLOC_ZERO);
            if (page == NULL) {
                return 0;
            }

            page->pp_ref++;
            new = (struct page_table *)KADDR(page2pa(page));
            *entry = PADDR(new) | PAGE_PRESENT | PAGE_USER | PAGE_WRITE;
        }
    }
    return 1;
}

/*
 * Given 'pml4', a pointer to a PML4, page_walk returns
 * a pointer to the page table entry (PTE) for linear address 'va'.
 * This requires walking the four-level page table structure.
 *
 * The relevant page table page might not exist yet.
 * If this is true, and create == false, then page_walk returns NULL.
 * Otherwise, page_walk allocates a new page table page with page_alloc.
 *    - If the allocation fails, page_walk returns NULL.
 *    - Otherwise, the new page's reference count is incremented,
 *  the page is cleared,
 *  and page_walk returns a pointer into the new page table page.
 *
 * Hint 1: you can turn a struct page_info* into the physical address of the
 * page it refers to with page2pa() from kern/pmap.h.
 *
 * Hint 2: the MMU checks permission bits in all the page table levels, so it
 * is safe to leave permissions in the page more permissive than strictly
 * necessary.
 *
 * Hint 3: look at inc/x86-64/paging.h for useful macros that manipulate page
 * table and page directory entries.
 */
physaddr_t *page_walk(struct page_table *pml4, const void *va, int create)
{
    struct page_info *page;
    struct page_table *pdp, *pd, *pt;
    physaddr_t *entry;

    // Pml4 entry. Check if exists, if not create new entry + table if possible
    entry = pml4->entries + PML4_INDEX((uintptr_t) va);
    if (!entry_in_table(entry, create)) {
        return NULL;
    }

    // PDP entry
    pdp = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = pdp->entries + PDPT_INDEX((uintptr_t) va);
    if (!entry_in_table(entry, create)) {
        return NULL;
    }

    // Page dir entry
    pd = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = pd->entries + PAGE_DIR_INDEX((uintptr_t) va);

    // Only for small pages, search for page table
    // If create == 0, we can still be searching for huge pages!
    if (create != CREATE_HUGE && !(!create && (*entry & PAGE_HUGE))) {
        if (!entry_in_table(entry, create)) {
            return NULL;
        }

        // Page table entry
        pt = (struct page_table *)KADDR(PAGE_ADDR(*entry));
        entry = pt->entries + PAGE_TABLE_INDEX((uintptr_t) va);
    } 

    // Page table or Page directory entry does not exist yet
    if (!(*entry & PAGE_PRESENT)) {
        // Can not create new page
        if (!create) {
            return NULL;
        }
        // Create new page entry
        else {
            *entry = 0;
        }
    }

    return entry;
}

/*
 * Map the physical page 'pp' at virtual address 'va'.
 * The permissions (the low 12 bits) of the page table entry
 * should be set to 'perm|PAGE_PRESENT'.
 *
 * Requirements
 *   - If there is already a page mapped at 'va', it should be page_remove()d.
 *   - If necessary, on demand, a page table should be allocated and inserted
 *     into 'pgdir'.
 *   - pp->pp_ref should be incremented if the insertion succeeds.
 *   - The TLB must be invalidated if a page was formerly present at 'va'.
 *
 * Corner-case hint: Make sure to consider what happens when the same
 * pp is re-inserted at the same virtual address in the same pgdir.
 * However, try not to distinguish this case in your code, as this
 * frequently leads to subtle bugs; there's an elegant way to handle
 * everything in one code path.
 *
 * RETURNS:
 *   0 on success
 *   -E_NO_MEM, if page table couldn't be allocated
 *
 * Hint: The TA solution is implemented using page_walk, page_remove,
 * and page2pa.
 */
int page_insert(struct page_table *pml4, struct page_info *pp, void *va, int perm)
{
    physaddr_t *addr;
    struct page_info *page;

    // Get page table entry
    if (pp->is_huge == 0) {
        addr = page_walk(pml4, va, CREATE_NORMAL);
    } else {
        addr = page_walk(pml4, va, CREATE_HUGE);
    }

    // Could not get page table entry for some reason, error
    if (addr == NULL) {
        return -E_NO_MEM;
    }

    // Check if entry has a mapping to a page or not
    if (*addr != 0) {
        // Page table entry already points to a page
        // Clear current link and link it to new page
        page_remove(pml4, va);
    }

    // Remove it from freelist if it was free before
    if (pp->pp_ref == 0 && pp->is_available == 1) {
        // First entry
        if (pp->previous == NULL) {
            page_free_list = page_free_list->pp_link;
            if (page_free_list != NULL) {
                page_free_list->previous = NULL;
            }
        }
        // Not first entry
        else {
            (pp->previous)->pp_link = pp->pp_link;
            if (pp->pp_link != NULL) {
                (pp->pp_link)->previous = pp->previous;
            }
        }

        pp->is_available = 0;
        pp->pp_link = NULL;
        pp->previous = NULL;
    }

    // Link page table entry to new page, PAGE_HUGE is handled via perm argument
    pp->pp_ref++;
    *addr = page2pa(pp) | perm | PAGE_PRESENT;
    return 0;
}

/*
 * Return the page mapped at virtual address 'va'.
 * If pte_store is not zero, then we store in it the address
 * of the pte for this page.  This is used by page_remove and
 * can be used to verify page permissions for syscall arguments,
 * but should not be used by most callers.
 *
 * Return NULL if there is no page mapped at va.
 *
 * Hint: the TA solution uses page_walk and pa2page.
 */
struct page_info *page_lookup(struct page_table *pml4, void *va,
    physaddr_t **entry_store)
{
    physaddr_t *addr;

    // Get address of page table entry
    addr = page_walk(pml4, va, 0);

    if (addr == NULL || *addr == 0) {
        return NULL;
    } else {
        // Save page table entry physical address
        if (entry_store != NULL) {
            *entry_store = addr;
        }

        // Get address of page and return page itself
        return pa2page(PAGE_ADDR(*addr));
    }
}

/*
 * Unmaps the physical page at virtual address 'va'.
 * If there is no physical page at that address, silently does nothing.
 *
 * Details:
 *   - The ref count on the physical page should decrement.
 *   - The physical page should be freed if the refcount reaches 0.
 *   - The pg table entry corresponding to 'va' should be set to 0.
 *     (if such a PTE exists)
 *   - The TLB must be invalidated if you remove an entry from
 *     the page table.
 *
 * Hint: The TA solution is implemented using page_lookup,
 *  tlb_invalidate, and page_decref.
 */
void page_remove(struct page_table *pml4, void *va)
{
    struct page_info *page;
    physaddr_t *pt_entry = NULL;

    // Get the page table of va
    page = page_lookup(pml4, va, &pt_entry);

    if (page == NULL) {
        return;
    }

    // Decrement link count of physical page, gets freed if it reaches 0
    page_decref(page);  

    // Set entry in pg table to 0
    *pt_entry = 0;

    // Invalidate tlb
    tlb_invalidate(pml4, va);
}

/*
 * Invalidate a TLB entry, but only if the page tables being
 * edited are the ones currently in use by the processor.
 */
void tlb_invalidate(struct page_table *pml4, void *va)
{
    /* Flush the entry only if we're modifying the current address space. */
    /* Note: for now, there is only one address space, so always invalidate. */
    flush_page(va);
}

static uintptr_t user_mem_check_addr;

/*
 * Check that an environment is allowed to access the range of memory
 * [va, va+len) with permissions 'perm | PTE_P'.
 * Normally 'perm' will contain PTE_U at least, but this is not required.
 * 'va' and 'len' need not be page-aligned; you must test every page that
 * contains any of that range.  You will test either 'len/PGSIZE',
 * 'len/PGSIZE + 1', or 'len/PGSIZE + 2' pages.
 *
 * A user program can access a virtual address if (1) the address is below
 * ULIM, and (2) the page table gives it permission.  These are exactly
 * the tests you should implement here.
 *
 * If there is an error, set the 'user_mem_check_addr' variable to the first
 * erroneous virtual address.
 *
 * Returns 0 if the user program can access this range of addresses,
 * and -E_FAULT otherwise.
 */
int user_mem_check(struct env *env, const void *va, size_t len, int perm)
{
    cprintf("[USER MEM CHECK] start\n");

    /* LAB 3: your code here. */
    physaddr_t *pt_entry;
    struct page_info *page;
    uintptr_t vi;
    uintptr_t va_p = (uintptr_t) va;
    uintptr_t va_start = ROUNDDOWN(va_p, PAGE_SIZE);
    uintptr_t va_end = ROUNDUP(va_p + len, PAGE_SIZE);

    for (vi = va_start; vi < va_end; vi += PAGE_SIZE) {
        pt_entry = NULL;
        page = NULL;

        // Get the page and its entry
        page = page_lookup(env->env_pml4, (void *)vi, &pt_entry);

        // Check if the page and entry is correct and user space
        if (page == NULL) {
            user_mem_check_addr = (vi >= va_p) ? vi : va_p;
            return -E_FAULT;
        }

        if (*pt_entry == 0) {
            user_mem_check_addr = (vi >= va_p) ? vi : va_p;
            return -E_FAULT;
        }
          
        if (*(unsigned long long int *)vi >= KERNEL_VMA) {
            user_mem_check_addr = (vi >= va_p) ? vi : va_p;
            return -E_FAULT;
        } 

        // Check permissions
        if (!(*pt_entry & PAGE_PRESENT && *pt_entry & perm)) {
            user_mem_check_addr = (vi >= va_p) ? vi : va_p;
            return -E_FAULT;
        }
    }

    return 0;
}

/*
 * Checks that environment 'env' is allowed to access the range
 * of memory [va, va+len) with permissions 'perm | PTE_U | PTE_P'.
 * If it can, then the function simply returns.
 * If it cannot, 'env' is destroyed and, if env is the current
 * environment, this function will not return.
 */
void user_mem_assert(struct env *env, const void *va, size_t len, int perm)
{
    if (user_mem_check(env, va, len, perm | PAGE_USER) < 0) {
        cprintf("[%08x] user_mem_check assertion failure for "
            "va %08x\n", env->env_id, user_mem_check_addr);
        env_destroy(env);   /* may not return */
    }
}

/***************************************************************
 * Checking functions.
 ***************************************************************/

/*
 * Check that the pages on the page_free_list are reasonable.
 */
static void check_page_free_list(bool only_low_memory)
{
    struct page_info *pp;
    physaddr_t limit = only_low_memory ? 0x400000 : 0xFFFFFFFF;
    int nfree_basemem = 0, nfree_extmem = 0;
    char *first_free_page;

    if (!page_free_list)
        panic("'page_free_list' is a null pointer!");

    if (only_low_memory) {
        /* Move pages with lower addresses first in the free list, since
         * entry_pgdir does not map all pages. */
        struct page_info *pp1, *pp2;
        struct page_info **tp[2] = { &pp1, &pp2 };
        for (pp = page_free_list; pp; pp = pp->pp_link) {
            int pagetype = page2pa(pp) >= limit;            // 1 if highmem, 0 if lowmem
            *tp[pagetype] = pp;
            tp[pagetype] = &pp->pp_link;
        }
        *tp[1] = 0;
        *tp[0] = pp2;
        page_free_list = pp1;

        // fix previous pointer
        struct page_info *prev = NULL;
        for (pp = page_free_list; pp; pp = pp->pp_link) {
            pp->previous = prev;
            prev = pp;
        }
    }

    /* if there's a page that shouldn't be on the free list,
     * try to make sure it eventually causes trouble. */
    for (pp = page_free_list; pp; pp = pp->pp_link)
        if (page2pa(pp) < limit)
            memset(page2kva(pp), 0x97, 128);

    first_free_page = (char *) boot_alloc(0);
    for (pp = page_free_list; pp; pp = pp->pp_link) {
        /* check that we didn't corrupt the free list itself */
        assert(pp >= pages);
        assert(pp < pages + npages);
        assert(((char *) pp - (char *) pages) % sizeof(*pp) == 0);

        /* check a few pages that shouldn't be on the free list */
        assert(page2pa(pp) != 0);
        assert(page2pa(pp) != IO_PHYS_MEM);
        assert(page2pa(pp) != EXT_PHYS_MEM - PAGE_SIZE);
        assert(page2pa(pp) != EXT_PHYS_MEM);
        assert(page2pa(pp) < EXT_PHYS_MEM || (char *) page2kva(pp) >=
            first_free_page);

        if (page2pa(pp) < EXT_PHYS_MEM)
            ++nfree_basemem;
        else
            ++nfree_extmem;
    }

    assert(nfree_basemem > 0);
    assert(nfree_extmem > 0);
}

/*
 * Check the physical page allocator (page_alloc(), page_free(),
 * and page_init()).
 */
static void check_page_alloc(void)
{
    struct page_info *pp, *pp0, *pp1, *pp2;
    struct page_info *php0, *php1, *php2;
    int nfree, total_free;
    struct page_info *fl;
    char *c;
    int i;

    if (!pages)
        panic("'pages' is a null pointer!");

    /* check number of free pages */
    for (pp = page_free_list, nfree = 0; pp; pp = pp->pp_link)
        ++nfree;

    total_free = nfree;

    /* should be able to allocate three pages */
    pp0 = pp1 = pp2 = 0;
    assert((pp0 = page_alloc(0)));
    assert((pp1 = page_alloc(0)));
    assert((pp2 = page_alloc(0)));

    assert(pp0);
    assert(pp1 && pp1 != pp0);
    assert(pp2 && pp2 != pp1 && pp2 != pp0);
    assert(page2pa(pp0) < npages*PAGE_SIZE);
    assert(page2pa(pp1) < npages*PAGE_SIZE);
    assert(page2pa(pp2) < npages*PAGE_SIZE);

    /* temporarily steal the rest of the free pages */
    fl = page_free_list;
    page_free_list = 0;

    /* should be no free memory */
    assert(!page_alloc(0));

    /* free and re-allocate? */
    page_free(pp0);
    page_free(pp1);
    page_free(pp2);
    pp0 = pp1 = pp2 = 0;
    assert((pp0 = page_alloc(0)));
    assert((pp1 = page_alloc(0)));
    assert((pp2 = page_alloc(0)));
    assert(pp0);
    assert(pp1 && pp1 != pp0);
    assert(pp2 && pp2 != pp1 && pp2 != pp0);
    assert(!page_alloc(0));

    /* test flags */
    memset(page2kva(pp0), 1, PAGE_SIZE);
    page_free(pp0);
    assert((pp = page_alloc(ALLOC_ZERO)));
    assert(pp && pp0 == pp);
    c = page2kva(pp);
    for (i = 0; i < PAGE_SIZE; i++)
        assert(c[i] == 0);

    /* give free list back */
    page_free_list = fl;

    /* free the pages we took */
    page_free(pp0);
    page_free(pp1);
    page_free(pp2);

    /* number of free pages should be the same */
    for (pp = page_free_list; pp; pp = pp->pp_link)
        --nfree;
    assert(nfree == 0);

    cprintf("[4K] check_page_alloc() succeeded!\n");

    /* test allocation of huge page */
    pp0 = pp1 = php0 = 0;
    assert((pp0 = page_alloc(0)));
    assert((php0 = page_alloc(ALLOC_HUGE)));
    assert((pp1 = page_alloc(0)));
    assert(pp0);
    assert(php0 && php0 != pp0);
    assert(pp1 && pp1 != php0 && pp1 != pp0);
    assert(0 == (page2pa(php0) % 512*PAGE_SIZE));
    if (page2pa(pp1) > page2pa(php0)) {
        assert(page2pa(pp1) - page2pa(php0) >= 512*PAGE_SIZE);
    }

    /* free and reallocate 2 huge pages */
    page_free(php0);
    page_free(pp0);
    page_free(pp1);
    php0 = php1 = pp0 = pp1 = 0;
    assert((php0 = page_alloc(ALLOC_HUGE)));
    assert((php1 = page_alloc(ALLOC_HUGE)));

    /* Is the inter-huge-page difference right? */
    if (page2pa(php1) > page2pa(php0)) {
        assert(page2pa(php1) - page2pa(php0) >= 512*PAGE_SIZE);
    } else {
        assert(page2pa(php0) - page2pa(php1) >= 512*PAGE_SIZE);
    }

    /* free the huge pages we took */
    page_free(php0);
    page_free(php1);

    /* number of free pages should be the same */
    nfree = total_free;
    for (pp = page_free_list; pp; pp = pp->pp_link)
        --nfree;
    assert(nfree == 0);

    cprintf("[2M] check_page_alloc() succeeded!\n");
}

/*
 * Checks that the kernel part of virtual address space
 * has been setup roughly correctly (by mem_init()).
 *
 * This function doesn't test every corner case,
 * but it is a pretty good sanity check.
 */
static void check_kern_pml4(void)
{
    uint32_t i, n;
    struct page_table *pml4;

    pml4 = kern_pml4;

    /* check pages array */
    n = ROUNDUP(npages*sizeof(struct page_info), PAGE_SIZE);
    for (i = 0; i < n; i += PAGE_SIZE) {
        assert(check_va2pa(pml4, USER_PAGES + i) == PADDR(pages) + i);
    }

    /* check envs array (new test for lab 3) */
    n = ROUNDUP(NENV*sizeof(struct env), PAGE_SIZE);
    for (i = 0; i < n; i += PAGE_SIZE)
        assert(check_va2pa(pml4, USER_ENVS + i) == PADDR(envs) + i);

    /* check phys mem */
    for (i = 0; i < npages * PAGE_SIZE; i += PAGE_SIZE)
        assert(check_va2pa(pml4, KERNEL_VMA + i) == i);

    /* check kernel stack */
    for (i = 0; i < KSTACK_SIZE; i += PAGE_SIZE)
        assert(check_va2pa(pml4, KSTACK_TOP - KSTACK_SIZE + i) == 
            (physaddr_t)bootstack + i);
    assert(check_va2pa(pml4, KSTACK_TOP - KSTACK_SIZE - PAGE_SIZE) == ~0);

    /* check PML permissions */
    for (i = 0; i < PAGE_TABLE_ENTRIES; i++) {
        switch (i) {
        case PML4_INDEX(USER_PML4):
        case PML4_INDEX(KSTACK_TOP-1):
        case PML4_INDEX(USER_PAGES):
        case PML4_INDEX(USER_ENVS):
        case PML4_INDEX(USER_VMAS):
            assert(pml4->entries[i] & PAGE_PRESENT);
            break;
        case PML4_INDEX(KERNEL_VMA):
            assert(pml4->entries[i] & PAGE_PRESENT);
            assert(pml4->entries[i] & PAGE_WRITE);
            break;
        default:
            assert(pml4->entries[i] == 0);
        }
    }
    cprintf("check_kern_pml4() succeeded!\n");
}

/*
 * This function returns the physical address of the page containing 'va',
 * defined by the page directory 'pgdir'.  The hardware normally performs
 * this functionality for us!  We define our own version to help check
 * the check_kern_pml4() function; it shouldn't be used elsewhere.
 */
static physaddr_t check_va2pa(struct page_table *pml4, uintptr_t va)
{
    struct page_table *pdpt, *page_dir, *page_table;
    physaddr_t *entry;

    entry = pml4->entries + PML4_INDEX(va);

    if (!(*entry & PAGE_PRESENT))
        return ~0;

    pdpt = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = pdpt->entries + PDPT_INDEX(va);

    if (!(*entry & PAGE_PRESENT))
        return ~0;

    page_dir = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = page_dir->entries + PAGE_DIR_INDEX(va);

    if (!(*entry & PAGE_PRESENT))
        return ~0;

    page_table = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = page_table->entries + PAGE_TABLE_INDEX(va);

    if (!(*entry & PAGE_PRESENT))
        return ~0;

    return PAGE_ADDR(*entry);
}

/* Check page_insert, page_remove, &c */
static void check_page(void)
{
    struct page_table *pdpt, *page_dir, *page_table;
    physaddr_t *entry, *entry2;
    struct page_info *pages[5];
    struct page_info *pp, *pp0, *pp1, *pp2, *tmp;
    struct page_info *fl;
    void *va;
    size_t i, j;
    //extern pde_t entry_pgdir[];

    /* should be able to allocate pages */
    for (i = 0; i < 5; ++i) {
        pages[i] = page_alloc(0);
    }

    for (i = 0; i < 5; ++i) {
        assert(pages[i]);

        for (j = 0; j < i; ++j) {
            assert(pages[i] != pages[j]);
        }
    }

    /* temporarily steal the rest of the free pages */
    fl = page_free_list;
    page_free_list = 0;

    /* should be no free memory */
    assert(!page_alloc(0));

    /* there is no page allocated at address 0 */
    assert(page_lookup(kern_pml4, (void *) 0x0, &entry2) == NULL);

    /* there is no free memory, so we can't allocate a page table */
    assert(page_insert(kern_pml4, pages[3], 0x0, PAGE_WRITE) < 0);

    /* free sufficient pages to allocate a pdpt, page directory and page table. */
    page_free(pages[0]);
    page_free(pages[1]);
    page_free(pages[2]);

    tmp = page_free_list;
    for (i = 0; i < 3; ++i) {
        tmp = tmp->pp_link;
    }

    assert(page_insert(kern_pml4, pages[3], 0x0, PAGE_WRITE) == 0);
    assert(check_va2pa(kern_pml4, 0x0) == page2pa(pages[3]));

    for (i = 0; i < 4; ++i) {
        assert(pages[i]->pp_ref == 1);
    }

    /* should be able to map pages[4] at PAGE_SIZE because there 
     * already is a page table. */
    assert(page_insert(kern_pml4, pages[4], (void*) PAGE_SIZE, PAGE_WRITE) == 0);
    assert(check_va2pa(kern_pml4, PAGE_SIZE) == page2pa(pages[4]));
    assert(pages[4]->pp_ref == 1);

    /* should be no free memory */
    assert(!page_alloc(0));

    /* should be able to map pages[4] at PAGE_SIZE because it's already there */
    assert(page_insert(kern_pml4, pages[4], (void*) PAGE_SIZE, PAGE_WRITE) == 0);
    assert(check_va2pa(kern_pml4, PAGE_SIZE) == page2pa(pages[4]));

    assert(pages[4]->pp_ref == 1);

    /* pages[4] should NOT be on the free list
     * could happen in ref counts are handled sloppily in page_insert */
    assert(!page_alloc(0));

    /* get a pointer to the entry in the page table. */
    entry = kern_pml4->entries;
    assert(*entry & PAGE_PRESENT);
    pdpt = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = pdpt->entries;
    assert(*entry & PAGE_PRESENT);
    page_dir = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = page_dir->entries;
    assert(*entry & PAGE_PRESENT);
    page_table = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = page_table->entries + PAGE_TABLE_INDEX(PAGE_SIZE);
    assert(*entry & PAGE_PRESENT);

    /* check that page_walk returns a pointer to the pte */
    assert(page_walk(kern_pml4, (void*)PAGE_SIZE, 0) == entry);

    /* should be able to change permissions too. */
    assert(page_insert(kern_pml4, pages[4], (void*) PAGE_SIZE, PAGE_WRITE |
        PAGE_USER) == 0);
    assert(check_va2pa(kern_pml4, PAGE_SIZE) == page2pa(pages[4]));
    assert(pages[4]->pp_ref == 1);
    assert(*page_walk(kern_pml4, (void*) PAGE_SIZE, 0) & PAGE_USER);
    assert(*entry & PAGE_USER);

    /* should be able to remap with fewer permissions */
    assert(page_insert(kern_pml4, pages[4], (void*) PAGE_SIZE, PAGE_WRITE) == 0);
    assert(*page_walk(kern_pml4, (void*) PAGE_SIZE, 0) & PAGE_WRITE);
    assert(!(*page_walk(kern_pml4, (void*) PAGE_SIZE, 0) & PAGE_USER));

    /* Should not be able to map at PTSIZE because need free page for page
     * table. */
    assert(page_insert(kern_pml4, pages[0], (void*)PAGE_TABLE_SPAN, PAGE_WRITE) < 0);

    /* insert pages[3] at PAGE_SIZE (replacing pages[4]) */
    assert(page_insert(kern_pml4, pages[3], (void*) PAGE_SIZE, PAGE_WRITE) == 0);
    assert(!(*page_walk(kern_pml4, (void*) PAGE_SIZE, 0) & PAGE_USER));

    /* should have pages[3] at both 0 and PAGE_SIZE, pages[4] nowhere, ... */
    assert(check_va2pa(kern_pml4, 0) == page2pa(pages[3]));
    assert(check_va2pa(kern_pml4, PAGE_SIZE) == page2pa(pages[3]));
    /* ... and ref counts should reflect this */
    assert(pages[3]->pp_ref == 2);
    assert(pages[4]->pp_ref == 0);

    /* pages[4] should be returned by page_alloc */
    assert((pp = page_alloc(0)) && pp == pages[4]);

    /* unmapping pp1 at 0 should keep pages[3] at PAGE_SIZE */
    page_remove(kern_pml4, 0x0);
    assert(check_va2pa(kern_pml4, 0x0) == ~0);
    assert(check_va2pa(kern_pml4, PAGE_SIZE) == page2pa(pages[3]));
    assert(pages[3]->pp_ref == 1);
    assert(pages[4]->pp_ref == 0);

    /* test re-inserting pages[3] at PAGE_SIZE */
    assert(page_insert(kern_pml4, pages[3], (void*) PAGE_SIZE, 0) == 0);
    assert(pages[3]->pp_ref);
    assert(pages[3]->pp_link == NULL);

    /* unmapping pages[3] at PAGE_SIZE should free it */
    page_remove(kern_pml4, (void*) PAGE_SIZE);
    assert(check_va2pa(kern_pml4, 0x0) == ~0);
    assert(check_va2pa(kern_pml4, PAGE_SIZE) == ~0);
    assert(pages[3]->pp_ref == 0);
    assert(pages[4]->pp_ref == 0);

    /* so it should be returned by page_alloc */
    assert((pp = page_alloc(0)) && pp == pages[3]);

    /* should be no free memory */
    assert(!page_alloc(0));

    /* forcibly take pages[0], pages[1] and pages[2] back */
    assert(PAGE_ADDR(kern_pml4->entries[0]) == page2pa(pages[2]));
    kern_pml4->entries[0] = 0;

    for (i = 0; i < 3; ++i) {
        assert(pages[i]->pp_ref == 1);
        pages[i]->pp_ref = 0;
        page_free(pages[i]);
    }

    /* check pointer arithmetic in page_walk */
    va = (void*)(PAGE_TABLE_SPAN + PAGE_SIZE);
    entry2 = page_walk(kern_pml4, va, 1);

    entry = kern_pml4->entries + PML4_INDEX((uintptr_t)va);
    assert(*entry & PAGE_PRESENT);
    pdpt = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = pdpt->entries + PDPT_INDEX((uintptr_t)va);
    assert(*entry & PAGE_PRESENT);
    page_dir = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = page_dir->entries + PAGE_DIR_INDEX((uintptr_t)va);
    assert(*entry & PAGE_PRESENT);
    page_table = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = page_table->entries + PAGE_TABLE_INDEX((uintptr_t)va);
    assert(entry == entry2);

    kern_pml4->entries[PML4_INDEX((uintptr_t)va)] = 0;

    for (i = 0; i < 3; ++i) {
        pages[i]->pp_ref = 0;
        page_free(pages[i]);
    }

    /* check that new page tables get cleared */
    memset(page2kva(pages[0]), 0xFF, PAGE_SIZE);
    page_walk(kern_pml4, 0x0, 1);

    entry = kern_pml4->entries;
    assert(*entry & PAGE_PRESENT);
    pdpt = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = pdpt->entries;
    assert(*entry & PAGE_PRESENT);
    page_dir = (struct page_table *)KADDR(PAGE_ADDR(*entry));
    entry = page_dir->entries;
    assert(*entry & PAGE_PRESENT);
    page_table = (struct page_table *)KADDR(PAGE_ADDR(*entry));

    for (i = 0; i < PAGE_TABLE_ENTRIES; ++i) {
        assert((page_table->entries[i] & PAGE_PRESENT) == 0);
    }

    kern_pml4->entries[0] = 0;

    for (i = 0; i < 3; ++i) {
        pages[i]->pp_ref = 0;
    }

    /* give free list back */
    page_free_list = fl;

    /* free the pages we took */
    for (i = 0; i < 5; ++i)
        page_free(pages[i]);

    cprintf("check_page() succeeded!\n");
}

/* Check page_insert, page_remove, &c, with an installed kern_pml4 */
static void check_page_installed_pml4(void)
{
    struct page_info *pp, *pp0, *pp1, *pp2;
    uintptr_t va;
    int i;

    /* check that we can read and write installed pages */
    pp1 = pp2 = 0;
    assert((pp0 = page_alloc(0)));
    assert((pp1 = page_alloc(0)));
    assert((pp2 = page_alloc(0)));
    page_free(pp0);
    memset(page2kva(pp1), 1, PAGE_SIZE);
    memset(page2kva(pp2), 2, PAGE_SIZE);
    page_insert(kern_pml4, pp1, (void*) PAGE_SIZE, PAGE_WRITE);
    assert(pp1->pp_ref == 1);
    assert(*(uint32_t *)PAGE_SIZE == 0x01010101U);
    page_insert(kern_pml4, pp2, (void*) PAGE_SIZE, PAGE_WRITE);
    assert(*(uint32_t *)PAGE_SIZE == 0x02020202U);
    assert(pp2->pp_ref == 1);
    assert(pp1->pp_ref == 0);
    *(uint32_t *)PAGE_SIZE = 0x03030303U;
    assert(*(uint32_t *)page2kva(pp2) == 0x03030303U);
    page_remove(kern_pml4, (void*) PAGE_SIZE);
    assert(pp2->pp_ref == 0);

    /* forcibly take pp0 back */
    assert(PAGE_ADDR(kern_pml4->entries[0]) == page2pa(pp0));
    kern_pml4->entries[0] = 0;
    assert(pp0->pp_ref == 1);
    pp0->pp_ref = 0;

    /* free the pages we took */
    page_free(pp0);

    cprintf("check_page_installed_pml4() succeeded!\n");
}


static void check_wx(void)
{
    struct page_table *pml4, *pdpt, *pgdir, *pt;
    size_t s, t, u, v;

    pml4 = kern_pml4;

    for (s = 0; s < PAGE_TABLE_ENTRIES; ++s) {
        if (s == PML4_INDEX(USER_PML4))
            continue;

        if (!(pml4->entries[s] & PAGE_PRESENT))
            continue;

        pdpt = (void *)(KERNEL_VMA + PAGE_ADDR(pml4->entries[s]));

        for (t = 0; t < PAGE_TABLE_ENTRIES; ++t) {
            if (!(pdpt->entries[t] & PAGE_PRESENT))
                continue;

            pgdir = (void *)(KERNEL_VMA + PAGE_ADDR(pdpt->entries[t]));

            for (u = 0; u < PAGE_TABLE_ENTRIES; ++u) {
                if (!(pgdir->entries[u] & PAGE_PRESENT))
                    continue;

                if ((pgdir->entries[u] & PAGE_HUGE)) {
                    if ((pgdir->entries[u] & (PAGE_NO_EXEC | PAGE_WRITE)) ==
                        PAGE_WRITE)
                        panic("page %016p is mapped both write and "
                            "executable!\n",
                            ((s >= 256) ? 0xffff800000000000 : 0) |
                            (s << PML4_SHIFT) | (t << PDPT_SHIFT) |
                            (u << PAGE_DIR_SHIFT));

                    continue;
                }

                pt = (void *)(KERNEL_VMA + PAGE_ADDR(pgdir->entries[u]));

                for (v = 0; v < PAGE_TABLE_ENTRIES; ++v) {
                    if (!(pt->entries[v] & PAGE_PRESENT))
                        continue;

                    if ((pt->entries[v] & (PAGE_NO_EXEC | PAGE_WRITE)) ==
                        PAGE_WRITE)
                        panic("page %016p is mapped both write and "
                            "executable!\n",
                            ((s >= 256) ? 0xffff800000000000 : 0) |
                            (s << PML4_SHIFT) | (t << PDPT_SHIFT) |
                            (u << PAGE_DIR_SHIFT) | (v << PAGE_TABLE_SHIFT));
                }
            }
        }
    }

    cprintf("check_wx() succeeded!\n");
}

/* Check page_walk() for huge page support */
static void check_page_hugepages(void)
{
    struct page_info *php0;
    assert(php0 = page_alloc(ALLOC_HUGE));
    assert(page_insert(kern_pml4, php0, (void *)(512*PAGE_SIZE), PAGE_WRITE |
        PAGE_HUGE) == 0);
    assert(php0->pp_ref == 1);
    memset(page2kva(php0), 1, PAGE_SIZE);
    assert(*(uint32_t *)(512*PAGE_SIZE) == 0x01010101U);

    /* Access the second 4K-page within the huge page */
    memset(page2kva(php0+1), 2, PAGE_SIZE);
    assert(*(uint32_t *)(513*PAGE_SIZE) == 0x02020202U);

    /* Writing to the last 4K-page within the huge page works? */
    *(uint32_t *)(2*512*PAGE_SIZE - 42) = 0x42424242U;
    assert(*(uint32_t *)(2*512*PAGE_SIZE - 42) == 0x42424242U);

    /* Are the underlying frames consecutive? */
    memset(page2kva(php0+509), 0x37, PAGE_SIZE);
    memset(page2kva(php0+510), 0x38, PAGE_SIZE);
    assert(*(uint32_t *)((512+509)*PAGE_SIZE) == 0x37373737U);
    assert(*(uint32_t *)((512+510)*PAGE_SIZE) == 0x38383838U);

    /* Check page_walk for the page and the PSE bit */
    physaddr_t *p_pte1, *p_pte2;
    p_pte1 = page_walk(kern_pml4, (void*)(512*PAGE_SIZE), 0);
    assert(NULL != p_pte1);
    assert(*p_pte1 & PAGE_HUGE);
    p_pte2 = page_walk(kern_pml4, (void*)(513*PAGE_SIZE), 0);
    assert(NULL != p_pte2);
    assert(p_pte1 == p_pte2);

    /* check page_remove() on the huge page */
    page_remove(kern_pml4, (void*) (512*PAGE_SIZE));
    assert(php0->pp_ref == 0);
    assert((php0+510)->pp_ref == 0);

    /* check CREATE_HUGE flag */
    p_pte1 = page_walk(kern_pml4, (void*)(512*PAGE_SIZE), CREATE_HUGE);
    assert(NULL != p_pte1);
    assert(php0 = page_alloc(ALLOC_HUGE));
    assert(page_insert(kern_pml4, php0, (void *)(2*512*PAGE_SIZE), PAGE_WRITE |
        PAGE_HUGE) == 0);
    page_remove(kern_pml4, (void*) (2*512*PAGE_SIZE));
    assert(php0->pp_ref == 0);

    cprintf("check_page_hugepages() succeeded!\n");
}
